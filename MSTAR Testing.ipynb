{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f577bbed",
   "metadata": {},
   "source": [
    "# MSTAR Testing\n",
    "\n",
    "encode_pretrained returns data and labels\n",
    "encode_dataset returns just data\n",
    "\n",
    "\n",
    "This is taking a while. I am going through this step by step and fixing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b77febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphlearning.active_learning as al\n",
    "import graphlearning as gl\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as torch_models\n",
    "import timeit\n",
    "import torch\n",
    "import random\n",
    "import models\n",
    "\n",
    "import utils\n",
    "import batch_active_learning as bal\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c61b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/Users/jameschapman/Documents/GitHub/SAR_BAL/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(bal)\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2581356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick one of 'mstar', 'open_sar', 'fusar'\n",
    "dataset_chosen = 'mstar'\n",
    "\n",
    "#This uses a CNNVAE to get embeddings\n",
    "# **I think this actually uses the fully trained and not a CNNVAE\n",
    "#Currently we always use the VAE for MSTAR since the knn_data is already stored for this\n",
    "\n",
    "use_fully_trained_features = True    #FIX THIS\n",
    "just_transfer              = False   #THIS IS WORKING\n",
    "transfer_and_train         = False   #This crashes due to computation on my computer\n",
    "\n",
    "assert(use_fully_trained_features + just_transfer + transfer_and_train == 1)\n",
    "\n",
    "#If you specify this, then it will use a specific trained NN for embeddings\n",
    "#If none, it will pick the ones deemed optimal from prior testing\n",
    "#  I recommend using None\n",
    "transfer_encoding = None\n",
    "\n",
    "#Determines the number of points in the coreset\n",
    "#Larger values correspond to smaller coresets\n",
    "density_radius_param = .5\n",
    "\n",
    "knn_num = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0012688e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n",
      "101\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'CVAE' on <module 'models' from '/Users/jameschapman/Documents/GitHub/SAR_BAL/models.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_chosen \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmstar\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_fully_trained_features:\n\u001b[0;32m---> 78\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencodeMSTAR\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/SAR10_CNNVAE.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m just_transfer:\n\u001b[1;32m     80\u001b[0m         X, labels \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mencode_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmstar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResNet\u001b[39m\u001b[38;5;124m'\u001b[39m, transformed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/GitHub/SAR_BAL/utils.py:721\u001b[0m, in \u001b[0;36mencodeMSTAR\u001b[0;34m(model_path, batch_size, cuda, use_phase)\u001b[0m\n\u001b[1;32m    717\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data)\u001b[38;5;241m.\u001b[39mdouble() \u001b[38;5;66;03m#Changed from float()\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m#Load model\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    723\u001b[0m encoded_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/serialization.py:1042\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'CVAE' on <module 'models' from '/Users/jameschapman/Documents/GitHub/SAR_BAL/models.py'>"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "with torch.no_grad():\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "if dataset_chosen == 'open_sar':\n",
    "  #Load labels\n",
    "  data, labels = utils.load_dataset('open_sar_ship', return_torch = False, concatenate = True)\n",
    "elif dataset_chosen == 'fusar':\n",
    "  #Load labels\n",
    "  data, labels = utils.load_dataset('fusar', return_torch = False, concatenate = True)\n",
    "elif dataset_chosen == 'mstar':\n",
    "  hdr, fields, mag, phase = utils.load_MSTAR('data/MSTAR/')\n",
    "  data = utils.polar_transform(mag, phase)\n",
    "  labels, target_names = utils.targets_to_labels(hdr)\n",
    "else:\n",
    "  assert False, \"Chosen dataset could not be loaded. Check for typos\"\n",
    "\n",
    "#Mimic that we know a percentage of data, and don't know for the rest\n",
    "#Do transfer learning merely using these\n",
    "percent_known_data = 0.05\n",
    "known_data_ind = gl.trainsets.generate(labels, rate=percent_known_data).tolist()\n",
    "known_data = data[known_data_ind]\n",
    "known_labels = labels[known_data_ind]\n",
    "\n",
    "# print(len(known_data))\n",
    "\n",
    "#Generate the initial set\n",
    "initial = gl.trainsets.generate(labels, rate=1).tolist()\n",
    "\n",
    "#Percent of known data to use as training data for transfer learning\n",
    "training_percent = 0.7\n",
    "transfer_train_ind = random.sample(range(len(known_data)), round(len(known_data)*training_percent))\n",
    "transfer_testing_ind = np.array([ind for ind in range(len(known_data)) if ind not in transfer_train_ind]).astype(int)\n",
    "\n",
    "#Convert to torch for use\n",
    "known_data = torch.from_numpy(known_data)\n",
    "known_labels = torch.from_numpy(known_labels)\n",
    "\n",
    "\n",
    "#print(len(transfer_testing_ind))\n",
    "training_data = known_data[transfer_train_ind]\n",
    "training_label = known_labels[transfer_train_ind]\n",
    "testing_data = known_data[transfer_testing_ind]\n",
    "testing_label = known_labels[transfer_testing_ind]\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(testing_data))\n",
    "\n",
    "data_info=[training_data, training_label, testing_data, testing_label]\n",
    "for item in data_info:\n",
    "  item = item.float()\n",
    "\n",
    "        \n",
    "if dataset_chosen == 'open_sar':\n",
    "    #Load encoded dataset\n",
    "    if use_fully_trained_features:\n",
    "        X = utils.encode_dataset('open_sar_ship','./models/open_sar_ship_CNN.pt')\n",
    "    elif just_transfer:\n",
    "        X, labels = utils.encode_pretrained('open_sar_ship', 'AlexNet', transformed=True)\n",
    "    else:\n",
    "        X = utils.encode_transfer_learning('open_sar_ship', model_type='AlexNet', transfer_batch_size=64, epochs=30, data_info=data_info)\n",
    "    #Load labels\n",
    "    _, labels = utils.load_dataset('open_sar_ship', return_torch = False, concatenate = True)\n",
    "    knn_data = gl.weightmatrix.knnsearch(X, knn_num, method='annoy', similarity='angular')\n",
    "elif dataset_chosen == 'fusar':\n",
    "    #Load encoded dataset\n",
    "    if use_fully_trained_features:\n",
    "        X = utils.encode_dataset('fusar','./models/fusar_CNN.pt')\n",
    "    elif just_transfer:\n",
    "        X, labels = utils.encode_pretrained('fusar', 'ShuffleNet', normalized=True, transformed=True)\n",
    "    else:\n",
    "        X = utils.encode_transfer_learning('fusar', model_type='ShuffleNet', transfer_batch_size=64, epochs=30, data_info=data_info)\n",
    "    #Load labels\n",
    "    _, labels = utils.load_dataset('fusar', return_torch = False, concatenate = True)\n",
    "    knn_data = gl.weightmatrix.knnsearch(X, knn_num, method='annoy', similarity='angular')\n",
    "elif dataset_chosen == 'mstar':\n",
    "    if use_fully_trained_features:\n",
    "        X = utils.encodeMSTAR('./models/SAR10_CNNVAE.pt', cuda=False)\n",
    "    elif just_transfer:\n",
    "        X, labels = utils.encode_pretrained('mstar', 'ResNet', transformed=False)\n",
    "    else:\n",
    "        X = utils.encode_transfer_learning('mstar', model_type = None, transfer_batch_size=64, epochs=10, data_info=data_info)\n",
    "    # X = utils.encode_transfer_learning(path, data_info=data_info, transformed=False)\n",
    "    \n",
    "    ##TODO: DUMB TESTING. COMMENT This out\n",
    "    #X = X[:500]\n",
    "    #labels = labels[:500]\n",
    "    knn_data = gl.weightmatrix.knnsearch(X, knn_num, method='annoy', similarity='angular')\n",
    "    \n",
    "    \n",
    "else:\n",
    "    assert False, \"Chosen dataset could not be loaded. Check for typos\"\n",
    "\n",
    "print(\"Constructing Graph Learning Objects\")\n",
    "W = gl.weightmatrix.knn(X, knn_num, kernel = 'gaussian', knn_data=knn_data)\n",
    "G = gl.graph(W)\n",
    "end = timeit.default_timer()\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "print(\"Complete\")\n",
    "print(f\"Time taken = {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941dca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the percent radius because it should be more robust across datasets\n",
    "coreset = bal.coreset_dijkstras(G, rad = .2, DEBUGGING=False, data = X, initial=initial, \n",
    "                                density_info = (True, density_radius_param, 1), knn_data=knn_data)\n",
    "print(\"Coreset Size = {}\\t Percent of data = {}%\".format(len(coreset), round(100 * len(coreset) / len(X), 2)))\n",
    "print(\"Coreset = \", coreset)\n",
    "print(labels[coreset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9290f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acq_fun_list = ['uc', 'vopt', 'mc', 'mcvopt']\n",
    "##TODO: Use the above one for actual testing\n",
    "acq_fun_list = ['uc']\n",
    "max_new_samples = 700\n",
    "#max_new_samples = 3060\n",
    "# max_new_samples = 2000\n",
    "batchsize=15\n",
    "\n",
    "L_time = []\n",
    "L_num_labels = []\n",
    "L_acc = []\n",
    "L_names = []\n",
    "for acq_fun in acq_fun_list:\n",
    "\n",
    "  num_iter = int(max_new_samples/batchsize)\n",
    "\n",
    "  al_mtd = 'local_max'\n",
    "  print(acq_fun, al_mtd)\n",
    "  start = timeit.default_timer() \n",
    "  _, list_num_labels, list_acc = bal.coreset_run_experiment(X, labels, W, coreset, num_iter=num_iter, method='Laplace',\n",
    "                           display=False, use_prior=False, al_mtd=al_mtd, debug=False,\n",
    "                           acq_fun=acq_fun, knn_data=knn_data, mtd_para=None,\n",
    "                           savefig=False, savefig_folder='../BAL_figures', batchsize=batchsize,\n",
    "                           dist_metric='angular', q=1, thresholding=0, randseed=0)\n",
    "  stop = timeit.default_timer()\n",
    "  L_time.append(stop - start)\n",
    "  L_num_labels.append(list_num_labels)\n",
    "  L_acc.append(list_acc)\n",
    "  L_names.append(al_mtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc48bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "fig = plt.figure()\n",
    "plt.plot(L_num_labels[i], L_acc[i])\n",
    "#plt.set_ylim([None, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a897e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
